{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adsamaz/Chattbot/blob/master/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5JEmmRx6Oju",
        "colab_type": "code",
        "outputId": "8a4d626d-0e2e-43bb-b57e-fbfba41fb3d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
        "from collections import Counter\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkUGX0Og6Yw0",
        "colab_type": "code",
        "outputId": "d782ca90-176b-4251-bc7a-cd99c22f711a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2EGAhBFoG3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_pandas(data, columns):\n",
        "    df_ = pd.DataFrame(columns=columns)\n",
        "    data['Sentence'] = data['Sentence'].str.lower()\n",
        "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
        "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
        "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
        "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
        "    for index, row in data.iterrows():\n",
        "        word_tokens = word_tokenize(row['Sentence'])\n",
        "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        df_ = df_.append({\n",
        "            \"index\": row['index'],\n",
        "            \"Class\": row['Class'],\n",
        "            \"Sentence\": \" \".join(filtered_sent[0:])\n",
        "        }, ignore_index=True)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gkpYOp3oCTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "8ec979ca-7e3d-4399-e90c-8ab33d9e3db6"
      },
      "source": [
        "# If this is the primary file that is executed (ie not an import of another file)\n",
        "if __name__ == \"__main__\":\n",
        "    # get data, pre-process and split\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/adsamaz/Chattbot/master/ANN_project_files/amazon_cells_labelled.txt?token=AL267CS3JPVMWZZ453JJJBC56NPZO\", delimiter='\\t', header=None)\n",
        "    data.columns = ['Sentence', 'Class']\n",
        "    data['index'] = data.index                                          # add new column index\n",
        "    columns = ['index', 'Class', 'Sentence']\n",
        "    data = preprocess_pandas(data, columns)                             # pre-process\n",
        "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
        "        data['Sentence'].values.astype('U'),\n",
        "        data['Class'].values.astype('int32'),\n",
        "        test_size=0.10,\n",
        "        random_state=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "    print(training_data.shape)\n",
        "\n",
        "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
        "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
        "    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
        "    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
        "    vocab_size = len(word_vectorizer.vocabulary_)\n",
        "    validation_data = word_vectorizer.transform(validation_data)\n",
        "    validation_data = validation_data.todense()\n",
        "    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
        "    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
        "    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
        "    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
        "\n",
        "    print(data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900,)\n",
            "                                              Sentence  Class  index\n",
            "0    so there is no way for me to plug it in here i...      0      0\n",
            "1                            good case excellent value      1      1\n",
            "2                                great for the jawbone      1      2\n",
            "3    tied to charger for conversations lasting more...      0      3\n",
            "4                                     the mic is great      1      4\n",
            "..                                                 ...    ...    ...\n",
            "995  the screen does get smudged easily because it ...      0    995\n",
            "996  what a piece of junk i lose more calls on this...      0    996\n",
            "997                        item does not match picture      0    997\n",
            "998  the only thing that disappoint me is the infra...      0    998\n",
            "999  you can not answer calls with the unit never w...      0    999\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTbWM5jrF8aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF('/content/drive/My Drive/Chatbot_shared/reviews_Video_Games_5.json.gz')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2a_lRBOIQQp",
        "colab_type": "code",
        "outputId": "ce59d60f-5159-4b17-e42f-3d70b2ebfa06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "#Removes everything in except the review and overall score. Then removes\n",
        "# all reviews with an score of 3 beacuse of them being natrual when we set all\n",
        "#score under 3 to 0(negative) and score over 3 to 1(positive)\n",
        "df=data\n",
        "df=df.get(['Sentence','Class'])\n",
        "#df=df[df.Class != 3.0] \n",
        "#df=df.replace(1.0, 0)\n",
        "#df=df.replace(2.0, 0)\n",
        "#df=df.replace(4.0, 1)\n",
        "#df=df.replace(5.0, 1)\n",
        "reviews = df.Sentence\n",
        "labels = df.Class\n",
        "df\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>so there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good case excellent value</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great for the jawbone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the mic is great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>the screen does get smudged easily because it ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>what a piece of junk i lose more calls on this...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>item does not match picture</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>the only thing that disappoint me is the infra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>you can not answer calls with the unit never w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  Class\n",
              "0    so there is no way for me to plug it in here i...      0\n",
              "1                            good case excellent value      1\n",
              "2                                great for the jawbone      1\n",
              "3    tied to charger for conversations lasting more...      0\n",
              "4                                     the mic is great      1\n",
              "..                                                 ...    ...\n",
              "995  the screen does get smudged easily because it ...      0\n",
              "996  what a piece of junk i lose more calls on this...      0\n",
              "997                        item does not match picture      0\n",
              "998  the only thing that disappoint me is the infra...      0\n",
              "999  you can not answer calls with the unit never w...      0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8hHHqLbSLaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removes symbols and lowercases all characters\n",
        "reviews = reviews.str.lower()\n",
        "reviews = reviews.str.replace(r'[^\\w\\s]+','')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LNGu47zUYpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6d3c515-77b0-4f18-c594-701caa7f1923"
      },
      "source": [
        "#Tokenizer\n",
        "all_text = ' '.join(reviews)          # Makes on string of all texts.\n",
        "words = all_text.split()\n",
        "\n",
        "#Count all the words\n",
        "count_words = Counter(words)          # Creats a counter function\n",
        "\n",
        "# Sort words after which is most common\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)\n",
        "sorted_words"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 513),\n",
              " ('i', 316),\n",
              " ('and', 310),\n",
              " ('it', 281),\n",
              " ('is', 243),\n",
              " ('a', 218),\n",
              " ('this', 206),\n",
              " ('to', 196),\n",
              " ('phone', 162),\n",
              " ('my', 143),\n",
              " ('for', 121),\n",
              " ('of', 120),\n",
              " ('not', 117),\n",
              " ('with', 112),\n",
              " ('very', 103),\n",
              " ('great', 97),\n",
              " ('was', 90),\n",
              " ('on', 89),\n",
              " ('in', 88),\n",
              " ('that', 80),\n",
              " ('good', 77),\n",
              " ('have', 73),\n",
              " ('you', 68),\n",
              " ('product', 55),\n",
              " ('quality', 49),\n",
              " ('had', 48),\n",
              " ('headset', 47),\n",
              " ('works', 47),\n",
              " ('but', 46),\n",
              " ('battery', 45),\n",
              " ('as', 45),\n",
              " ('its', 43),\n",
              " ('so', 42),\n",
              " ('are', 42),\n",
              " ('sound', 41),\n",
              " ('all', 41),\n",
              " ('use', 41),\n",
              " ('one', 40),\n",
              " ('well', 38),\n",
              " ('ear', 35),\n",
              " ('has', 34),\n",
              " ('would', 34),\n",
              " ('work', 34),\n",
              " ('from', 33),\n",
              " ('your', 32),\n",
              " ('dont', 31),\n",
              " ('like', 30),\n",
              " ('case', 29),\n",
              " ('if', 29),\n",
              " ('me', 28),\n",
              " ('than', 28),\n",
              " ('be', 28),\n",
              " ('ive', 28),\n",
              " ('excellent', 27),\n",
              " ('time', 27),\n",
              " ('after', 27),\n",
              " ('price', 27),\n",
              " ('no', 26),\n",
              " ('up', 26),\n",
              " ('recommend', 26),\n",
              " ('does', 26),\n",
              " ('really', 26),\n",
              " ('at', 24),\n",
              " ('im', 24),\n",
              " ('or', 23),\n",
              " ('best', 23),\n",
              " ('service', 23),\n",
              " ('get', 22),\n",
              " ('when', 22),\n",
              " ('out', 22),\n",
              " ('nice', 22),\n",
              " ('only', 22),\n",
              " ('also', 22),\n",
              " ('too', 21),\n",
              " ('just', 21),\n",
              " ('any', 20),\n",
              " ('new', 20),\n",
              " ('love', 20),\n",
              " ('these', 20),\n",
              " ('worked', 20),\n",
              " ('am', 20),\n",
              " ('charger', 19),\n",
              " ('more', 19),\n",
              " ('money', 19),\n",
              " ('do', 19),\n",
              " ('can', 19),\n",
              " ('buy', 19),\n",
              " ('item', 19),\n",
              " ('better', 19),\n",
              " ('an', 19),\n",
              " ('ever', 19),\n",
              " ('car', 18),\n",
              " ('about', 18),\n",
              " ('even', 18),\n",
              " ('because', 18),\n",
              " ('then', 17),\n",
              " ('what', 17),\n",
              " ('comfortable', 17),\n",
              " ('bought', 17),\n",
              " ('now', 17),\n",
              " ('first', 17),\n",
              " ('bluetooth', 17),\n",
              " ('they', 17),\n",
              " ('easy', 17),\n",
              " ('could', 16),\n",
              " ('doesnt', 16),\n",
              " ('did', 16),\n",
              " ('used', 16),\n",
              " ('poor', 15),\n",
              " ('been', 15),\n",
              " ('happy', 15),\n",
              " ('which', 15),\n",
              " ('will', 15),\n",
              " ('reception', 15),\n",
              " ('there', 14),\n",
              " ('waste', 14),\n",
              " ('two', 14),\n",
              " ('made', 14),\n",
              " ('still', 14),\n",
              " ('off', 14),\n",
              " ('bad', 14),\n",
              " ('purchase', 14),\n",
              " ('few', 14),\n",
              " ('cell', 14),\n",
              " ('while', 14),\n",
              " ('worst', 14),\n",
              " ('them', 13),\n",
              " ('far', 13),\n",
              " ('charge', 13),\n",
              " ('fine', 13),\n",
              " ('calls', 13),\n",
              " ('enough', 13),\n",
              " ('thing', 13),\n",
              " ('device', 13),\n",
              " ('piece', 13),\n",
              " ('got', 13),\n",
              " ('same', 13),\n",
              " ('problems', 12),\n",
              " ('right', 12),\n",
              " ('volume', 12),\n",
              " ('long', 12),\n",
              " ('again', 12),\n",
              " ('problem', 12),\n",
              " ('hear', 12),\n",
              " ('make', 12),\n",
              " ('life', 12),\n",
              " ('camera', 12),\n",
              " ('clear', 12),\n",
              " ('phones', 12),\n",
              " ('much', 12),\n",
              " ('using', 12),\n",
              " ('fit', 12),\n",
              " ('plug', 11),\n",
              " ('design', 11),\n",
              " ('makes', 11),\n",
              " ('couldnt', 11),\n",
              " ('other', 11),\n",
              " ('working', 11),\n",
              " ('motorola', 11),\n",
              " ('fits', 11),\n",
              " ('into', 10),\n",
              " ('think', 10),\n",
              " ('people', 10),\n",
              " ('disappointed', 10),\n",
              " ('looks', 10),\n",
              " ('call', 10),\n",
              " ('over', 10),\n",
              " ('terrible', 10),\n",
              " ('impressed', 9),\n",
              " ('highly', 9),\n",
              " ('how', 9),\n",
              " ('days', 9),\n",
              " ('months', 9),\n",
              " ('pretty', 9),\n",
              " ('everything', 9),\n",
              " ('cool', 9),\n",
              " ('wear', 9),\n",
              " ('lot', 9),\n",
              " ('jabra', 9),\n",
              " ('screen', 9),\n",
              " ('cheap', 9),\n",
              " ('customer', 9),\n",
              " ('low', 9),\n",
              " ('amazon', 9),\n",
              " ('by', 8),\n",
              " ('last', 8),\n",
              " ('without', 8),\n",
              " ('talk', 8),\n",
              " ('little', 8),\n",
              " ('years', 8),\n",
              " ('never', 8),\n",
              " ('their', 8),\n",
              " ('verizon', 8),\n",
              " ('buttons', 8),\n",
              " ('headsets', 8),\n",
              " ('broke', 8),\n",
              " ('found', 8),\n",
              " ('tried', 8),\n",
              " ('light', 8),\n",
              " ('small', 8),\n",
              " ('voice', 8),\n",
              " ('look', 8),\n",
              " ('being', 8),\n",
              " ('back', 8),\n",
              " ('horrible', 8),\n",
              " ('year', 8),\n",
              " ('junk', 8),\n",
              " ('unit', 8),\n",
              " ('however', 8),\n",
              " ('way', 7),\n",
              " ('several', 7),\n",
              " ('say', 7),\n",
              " ('went', 7),\n",
              " ('most', 7),\n",
              " ('didnt', 7),\n",
              " ('audio', 7),\n",
              " ('dropped', 7),\n",
              " ('we', 7),\n",
              " ('since', 7),\n",
              " ('big', 7),\n",
              " ('gets', 7),\n",
              " ('down', 7),\n",
              " ('both', 7),\n",
              " ('completely', 7),\n",
              " ('software', 7),\n",
              " ('some', 7),\n",
              " ('useless', 7),\n",
              " ('company', 7),\n",
              " ('nokia', 7),\n",
              " ('quite', 7),\n",
              " ('cant', 7),\n",
              " ('looking', 7),\n",
              " ('take', 7),\n",
              " ('go', 6),\n",
              " ('going', 6),\n",
              " ('simple', 6),\n",
              " ('ears', 6),\n",
              " ('need', 6),\n",
              " ('picture', 6),\n",
              " ('want', 6),\n",
              " ('loud', 6),\n",
              " ('end', 6),\n",
              " ('headphones', 6),\n",
              " ('find', 6),\n",
              " ('within', 6),\n",
              " ('received', 6),\n",
              " ('black', 6),\n",
              " ('around', 6),\n",
              " ('signal', 6),\n",
              " ('perfectly', 6),\n",
              " ('internet', 6),\n",
              " ('less', 6),\n",
              " ('put', 6),\n",
              " ('every', 6),\n",
              " ('stay', 6),\n",
              " ('before', 6),\n",
              " ('cable', 6),\n",
              " ('real', 6),\n",
              " ('came', 6),\n",
              " ('anyone', 6),\n",
              " ('difficult', 6),\n",
              " ('value', 5),\n",
              " ('razr', 5),\n",
              " ('original', 5),\n",
              " ('started', 5),\n",
              " ('thats', 5),\n",
              " ('three', 5),\n",
              " ('charging', 5),\n",
              " ('helpful', 5),\n",
              " ('hold', 5),\n",
              " ('sure', 5),\n",
              " ('turn', 5),\n",
              " ('priced', 5),\n",
              " ('sturdy', 5),\n",
              " ('different', 5),\n",
              " ('week', 5),\n",
              " ('feels', 5),\n",
              " ('arrived', 5),\n",
              " ('quickly', 5),\n",
              " ('expect', 5),\n",
              " ('definitely', 5),\n",
              " ('shipping', 5),\n",
              " ('pictures', 5),\n",
              " ('high', 5),\n",
              " ('strong', 5),\n",
              " ('pleased', 5),\n",
              " ('job', 5),\n",
              " ('weeks', 5),\n",
              " ('hours', 5),\n",
              " ('know', 5),\n",
              " ('kind', 5),\n",
              " ('charm', 5),\n",
              " ('awesome', 5),\n",
              " ('where', 5),\n",
              " ('minutes', 5),\n",
              " ('color', 5),\n",
              " ('part', 5),\n",
              " ('range', 5),\n",
              " ('disappointment', 5),\n",
              " ('important', 5),\n",
              " ('return', 5),\n",
              " ('hands', 5),\n",
              " ('feature', 5),\n",
              " ('many', 5),\n",
              " ('old', 5),\n",
              " ('anything', 5),\n",
              " ('couple', 5),\n",
              " ('disappointing', 5),\n",
              " ('belt', 5),\n",
              " ('data', 5),\n",
              " ('having', 5),\n",
              " ('others', 5),\n",
              " ('crap', 5),\n",
              " ('especially', 5),\n",
              " ('keep', 5),\n",
              " ('plastic', 5),\n",
              " ('clarity', 5),\n",
              " ('hard', 5),\n",
              " ('always', 5),\n",
              " ('none', 5),\n",
              " ('cannot', 5),\n",
              " ('samsung', 5),\n",
              " ('cases', 5),\n",
              " ('connection', 5),\n",
              " ('easily', 5),\n",
              " ('here', 4),\n",
              " ('mic', 4),\n",
              " ('decent', 4),\n",
              " ('sending', 4),\n",
              " ('must', 4),\n",
              " ('were', 4),\n",
              " ('clip', 4),\n",
              " ('blue', 4),\n",
              " ('place', 4),\n",
              " ('absolutely', 4),\n",
              " ('bars', 4),\n",
              " ('she', 4),\n",
              " ('instructions', 4),\n",
              " ('left', 4),\n",
              " ('hate', 4),\n",
              " ('kept', 4),\n",
              " ('performance', 4),\n",
              " ('seems', 4),\n",
              " ('should', 4),\n",
              " ('keyboard', 4),\n",
              " ('actually', 4),\n",
              " ('support', 4),\n",
              " ('player', 4),\n",
              " ('later', 4),\n",
              " ('purchased', 4),\n",
              " ('holds', 4),\n",
              " ('bargain', 4),\n",
              " ('order', 4),\n",
              " ('free', 4),\n",
              " ('leather', 4),\n",
              " ('fast', 4),\n",
              " ('comfortably', 4),\n",
              " ('bt', 4),\n",
              " ('tool', 4),\n",
              " ('obviously', 4),\n",
              " ('side', 4),\n",
              " ('lasts', 4),\n",
              " ('able', 4),\n",
              " ('lightweight', 4),\n",
              " ('id', 4),\n",
              " ('expected', 4),\n",
              " ('mistake', 4),\n",
              " ('worth', 4),\n",
              " ('earpiece', 4),\n",
              " ('either', 4),\n",
              " ('unreliable', 4),\n",
              " ('family', 4),\n",
              " ('seller', 4),\n",
              " ('plantronics', 4),\n",
              " ('buying', 4),\n",
              " ('weak', 4),\n",
              " ('lg', 4),\n",
              " ('sucks', 4),\n",
              " ('ago', 4),\n",
              " ('times', 4),\n",
              " ('easier', 4),\n",
              " ('face', 4),\n",
              " ('wanted', 4),\n",
              " ('deal', 4),\n",
              " ('satisfied', 4),\n",
              " ('comes', 4),\n",
              " ('another', 4),\n",
              " ('rather', 4),\n",
              " ('away', 4),\n",
              " ('nothing', 4),\n",
              " ('tmobile', 4),\n",
              " ('unfortunately', 4),\n",
              " ('those', 4),\n",
              " ('store', 4),\n",
              " ('replace', 4),\n",
              " ('own', 4),\n",
              " ('ringtones', 4),\n",
              " ('said', 4),\n",
              " ('day', 4),\n",
              " ('treo', 4),\n",
              " ('extra', 4),\n",
              " ('awful', 4),\n",
              " ('jawbone', 3),\n",
              " ('conversations', 3),\n",
              " ('line', 3),\n",
              " ('contacts', 3),\n",
              " ('static', 3),\n",
              " ('though', 3),\n",
              " ('who', 3),\n",
              " ('everyone', 3),\n",
              " ('pair', 3),\n",
              " ('yet', 3),\n",
              " ('below', 3),\n",
              " ('pocket', 3),\n",
              " ('pc', 3),\n",
              " ('provided', 3),\n",
              " ('included', 3),\n",
              " ('worthless', 3),\n",
              " ('features', 3),\n",
              " ('protection', 3),\n",
              " ('instead', 3),\n",
              " ('seconds', 3),\n",
              " ('perhaps', 3),\n",
              " ('seriously', 3),\n",
              " ('front', 3),\n",
              " ('trouble', 3),\n",
              " ('choice', 3),\n",
              " ('home', 3),\n",
              " ('beautiful', 3),\n",
              " ('longer', 3),\n",
              " ('packaged', 3),\n",
              " ('construction', 3),\n",
              " ('super', 3),\n",
              " ('ease', 3),\n",
              " ('plan', 3),\n",
              " ('decision', 3),\n",
              " ('match', 3),\n",
              " ('between', 3),\n",
              " ('fall', 3),\n",
              " ('such', 3),\n",
              " ('wife', 3),\n",
              " ('display', 3),\n",
              " ('rocks', 3),\n",
              " ('may', 3),\n",
              " ('setup', 3),\n",
              " ('earpieces', 3),\n",
              " ('set', 3),\n",
              " ('getting', 3),\n",
              " ('almost', 3),\n",
              " ('avoid', 3),\n",
              " ('earbud', 3),\n",
              " ('failed', 3),\n",
              " ('coverage', 3),\n",
              " ('drops', 3),\n",
              " ('area', 3),\n",
              " ('forever', 3),\n",
              " ('glad', 3),\n",
              " ('description', 3),\n",
              " ('s', 3),\n",
              " ('fantastic', 3),\n",
              " ('sharp', 3),\n",
              " ('chargers', 3),\n",
              " ('network', 3),\n",
              " ('slow', 3),\n",
              " ('once', 3),\n",
              " ('lost', 3),\n",
              " ('replacement', 3),\n",
              " ('extremely', 3),\n",
              " ('simply', 3),\n",
              " ('thought', 3),\n",
              " ('reasonably', 3),\n",
              " ('form', 3),\n",
              " ('experience', 3),\n",
              " ('theres', 3),\n",
              " ('ordered', 3),\n",
              " ('sony', 3),\n",
              " ('market', 3),\n",
              " ('comfort', 3),\n",
              " ('probably', 3),\n",
              " ('drain', 3),\n",
              " ('poorly', 3),\n",
              " ('charged', 3),\n",
              " ('scratched', 3),\n",
              " ('microphone', 3),\n",
              " ('care', 3),\n",
              " ('lacking', 3),\n",
              " ('uncomfortable', 3),\n",
              " ('plugged', 3),\n",
              " ('flip', 3),\n",
              " ('wireless', 3),\n",
              " ('happier', 3),\n",
              " ('ill', 3),\n",
              " ('try', 3),\n",
              " ('trying', 3),\n",
              " ('handsfree', 3),\n",
              " ('finally', 3),\n",
              " ('give', 3),\n",
              " ('goes', 3),\n",
              " ('dead', 3),\n",
              " ('overall', 3),\n",
              " ('might', 3),\n",
              " ('q', 3),\n",
              " ('size', 3),\n",
              " ('wasnt', 3),\n",
              " ('expensive', 3),\n",
              " ('turned', 3),\n",
              " ('computer', 3),\n",
              " ('under', 3),\n",
              " ('wont', 3),\n",
              " ('results', 3),\n",
              " ('noise', 3),\n",
              " ('given', 3),\n",
              " ('holster', 3),\n",
              " ('palm', 3),\n",
              " ('refund', 3),\n",
              " ('drop', 3),\n",
              " ('through', 3),\n",
              " ('speaker', 3),\n",
              " ('sprint', 3),\n",
              " ('feel', 3),\n",
              " ('needed', 3),\n",
              " ('usb', 3),\n",
              " ('plus', 3),\n",
              " ('pay', 3),\n",
              " ('tinny', 3),\n",
              " ('pairing', 3),\n",
              " ('iphone', 3),\n",
              " ('despite', 3),\n",
              " ('outlet', 3),\n",
              " ('cingular', 3),\n",
              " ('break', 3),\n",
              " ('beep', 3),\n",
              " ('us', 2),\n",
              " ('unless', 2),\n",
              " ('lasting', 2),\n",
              " ('wasted', 2),\n",
              " ('he', 2),\n",
              " ('extended', 2),\n",
              " ('notice', 2),\n",
              " ('tooth', 2),\n",
              " ('advise', 2),\n",
              " ('motorolas', 2),\n",
              " ('website', 2),\n",
              " ('fire', 2),\n",
              " ('run', 2),\n",
              " ('owned', 2),\n",
              " ('mobile', 2),\n",
              " ('pull', 2),\n",
              " ('unusable', 2),\n",
              " ('least', 2),\n",
              " ('book', 2),\n",
              " ('mp', 2),\n",
              " ('regarding', 2),\n",
              " ('returned', 2),\n",
              " ('turns', 2),\n",
              " ('pda', 2),\n",
              " ('large', 2),\n",
              " ('essentially', 2),\n",
              " ('forget', 2),\n",
              " ('tech', 2),\n",
              " ('particular', 2),\n",
              " ('party', 2),\n",
              " ('clearly', 2),\n",
              " ('cover', 2),\n",
              " ('let', 2),\n",
              " ('lock', 2),\n",
              " ('died', 2),\n",
              " ('glasses', 2),\n",
              " ('sometimes', 2),\n",
              " ('series', 2),\n",
              " ('quiet', 2),\n",
              " ('saying', 2),\n",
              " ('docking', 2),\n",
              " ('station', 2),\n",
              " ('advertised', 2),\n",
              " ('handy', 2),\n",
              " ('loves', 2),\n",
              " ('cheaper', 2),\n",
              " ('costs', 2),\n",
              " ('play', 2),\n",
              " ('music', 2),\n",
              " ('beware', 2),\n",
              " ('white', 2),\n",
              " ('huge', 2),\n",
              " ('flaw', 2),\n",
              " ('although', 2),\n",
              " ('impressive', 2),\n",
              " ('resolution', 2),\n",
              " ('ask', 2),\n",
              " ('slim', 2),\n",
              " ('sex', 2),\n",
              " ('sleek', 2),\n",
              " ('full', 2),\n",
              " ('number', 2),\n",
              " ('keypad', 2),\n",
              " ('unhappy', 2),\n",
              " ('done', 2),\n",
              " ('basically', 2),\n",
              " ('careful', 2),\n",
              " ('logitech', 2),\n",
              " ('stuff', 2),\n",
              " ('house', 2),\n",
              " ('recognition', 2),\n",
              " ('tremendous', 2),\n",
              " ('during', 2),\n",
              " ('experienced', 2),\n",
              " ('takes', 2),\n",
              " ('literally', 2),\n",
              " ('stated', 2),\n",
              " ('hoping', 2),\n",
              " ('blackberry', 2),\n",
              " ('sounds', 2),\n",
              " ('technology', 2),\n",
              " ('wouldnt', 2),\n",
              " ('wired', 2),\n",
              " ('previous', 2),\n",
              " ('wi', 2),\n",
              " ('superb', 2),\n",
              " ('maintain', 2),\n",
              " ('graphics', 2),\n",
              " ('button', 2),\n",
              " ('thank', 2),\n",
              " ('igo', 2),\n",
              " ('tips', 2),\n",
              " ('connected', 2),\n",
              " ('wifes', 2),\n",
              " ('storage', 2),\n",
              " ('buzzing', 2),\n",
              " ('override', 2),\n",
              " ('functionality', 2),\n",
              " ('incredible', 2),\n",
              " ('ring', 2),\n",
              " ('dropping', 2),\n",
              " ('thin', 2),\n",
              " ('nearly', 2),\n",
              " ('bother', 2),\n",
              " ('room', 2),\n",
              " ('issues', 2),\n",
              " ('felt', 2),\n",
              " ('embarrassing', 2),\n",
              " ('consumer', 2),\n",
              " ('background', 2),\n",
              " ('certainly', 2),\n",
              " ('usually', 2),\n",
              " ('mess', 2),\n",
              " ('bit', 2),\n",
              " ('tell', 2),\n",
              " ('excited', 2),\n",
              " ('additional', 2),\n",
              " ('gels', 2),\n",
              " ('whatsoever', 2),\n",
              " ('secure', 2),\n",
              " ('o', 2),\n",
              " ('appears', 2),\n",
              " ('smell', 2),\n",
              " ('caused', 2),\n",
              " ('flimsy', 2),\n",
              " ('month', 2),\n",
              " ('flawlessly', 2),\n",
              " ('stars', 2),\n",
              " ('whole', 2),\n",
              " ('adorable', 2),\n",
              " ('vi', 2),\n",
              " ('gotten', 2),\n",
              " ('driving', 2),\n",
              " ('dialing', 2),\n",
              " ('neither', 2),\n",
              " ('games', 2),\n",
              " ('ipod', 2),\n",
              " ('recharge', 2),\n",
              " ('save', 2),\n",
              " ('along', 2),\n",
              " ('starts', 2),\n",
              " ('ringing', 2),\n",
              " ('reason', 2),\n",
              " ('push', 2),\n",
              " ('sides', 2),\n",
              " ('skype', 2),\n",
              " ('shipped', 2),\n",
              " ('exactly', 2),\n",
              " ('waiting', 2),\n",
              " ('stupid', 2),\n",
              " ('noticed', 2),\n",
              " ('att', 2),\n",
              " ('breaks', 2),\n",
              " ('oh', 2),\n",
              " ('effect', 2),\n",
              " ('model', 2),\n",
              " ('warning', 2),\n",
              " ('dying', 2),\n",
              " ('alone', 2),\n",
              " ('install', 2),\n",
              " ('purchasing', 2),\n",
              " ('moto', 2),\n",
              " ('figure', 2),\n",
              " ('reading', 2),\n",
              " ('wearing', 2),\n",
              " ('sunglasses', 2),\n",
              " ('returning', 2),\n",
              " ('cumbersome', 2),\n",
              " ('vx', 2),\n",
              " ('switch', 2),\n",
              " ('worthwhile', 2),\n",
              " ('understand', 2),\n",
              " ('batteries', 2),\n",
              " ('user', 2),\n",
              " ('friendly', 2),\n",
              " ('ability', 2),\n",
              " ('receiving', 2),\n",
              " ('exchanged', 2),\n",
              " ('wrong', 2),\n",
              " ('described', 2),\n",
              " ('defective', 2),\n",
              " ('star', 2),\n",
              " ('review', 2),\n",
              " ('catching', 2),\n",
              " ('amazed', 2),\n",
              " ('timeframe', 2),\n",
              " ('complaint', 2),\n",
              " ('things', 2),\n",
              " ('ended', 2),\n",
              " ('accidentally', 2),\n",
              " ('listening', 2),\n",
              " ('took', 2),\n",
              " ('above', 2),\n",
              " ('conversation', 2),\n",
              " ('eargels', 2),\n",
              " ('seem', 2),\n",
              " ('numerous', 2),\n",
              " ('please', 2),\n",
              " ('barely', 2),\n",
              " ('joke', 2),\n",
              " ('forced', 2),\n",
              " ('holding', 2),\n",
              " ('breaking', 2),\n",
              " ('quick', 2),\n",
              " ('e', 2),\n",
              " ('operate', 2),\n",
              " ('paired', 2),\n",
              " ('come', 2),\n",
              " ('brand', 2),\n",
              " ('red', 2),\n",
              " ('reviews', 2),\n",
              " ('echo', 2),\n",
              " ('told', 2),\n",
              " ('warranty', 2),\n",
              " ('something', 2),\n",
              " ('bar', 2),\n",
              " ('placed', 2),\n",
              " ('spring', 2),\n",
              " ('tries', 2),\n",
              " ('download', 2),\n",
              " ('access', 2),\n",
              " ('third', 2),\n",
              " ('flash', 2),\n",
              " ('tones', 2),\n",
              " ('chinese', 2),\n",
              " ('crisp', 2),\n",
              " ('video', 2),\n",
              " ('hour', 2),\n",
              " ('accept', 2),\n",
              " ('allows', 2),\n",
              " ('power', 2),\n",
              " ('wall', 2),\n",
              " ('etc', 2),\n",
              " ('hand', 2),\n",
              " ('cut', 2),\n",
              " ('sizes', 2),\n",
              " ('next', 2),\n",
              " ('protector', 2),\n",
              " ('date', 2),\n",
              " ('sounded', 2),\n",
              " ('together', 2),\n",
              " ('feet', 2),\n",
              " ('send', 2),\n",
              " ('current', 2),\n",
              " ('says', 2),\n",
              " ('laptop', 2),\n",
              " ('inside', 2),\n",
              " ('normal', 2),\n",
              " ('making', 2),\n",
              " ('fails', 2),\n",
              " ('lose', 2),\n",
              " ('ok', 2),\n",
              " ('wow', 2),\n",
              " ('converter', 1),\n",
              " ('tied', 1),\n",
              " ('minutesmajor', 1),\n",
              " ('jiggle', 1),\n",
              " ('dozen', 1),\n",
              " ('hundred', 1),\n",
              " ('imagine', 1),\n",
              " ('fun', 1),\n",
              " ('each', 1),\n",
              " ('owneryou', 1),\n",
              " ('needless', 1),\n",
              " ('seperated', 1),\n",
              " ('mere', 1),\n",
              " ('ft', 1),\n",
              " ('excessive', 1),\n",
              " ('garbled', 1),\n",
              " ('odd', 1),\n",
              " ('fooled', 1),\n",
              " ('clicks', 1),\n",
              " ('wonder', 1),\n",
              " ('mechanism', 1),\n",
              " ('followed', 1),\n",
              " ('directions', 1),\n",
              " ('kindle', 1),\n",
              " ('loved', 1),\n",
              " ('commercials', 1),\n",
              " ('misleading', 1),\n",
              " ('mother', 1),\n",
              " ('combination', 1),\n",
              " ('earphone', 1),\n",
              " ('breakage', 1),\n",
              " ('unacceptible', 1),\n",
              " ('ideal', 1),\n",
              " ('whose', 1),\n",
              " ('sensitive', 1),\n",
              " ('moving', 1),\n",
              " ('freeway', 1),\n",
              " ('speed', 1),\n",
              " ('contract', 1),\n",
              " ('ac', 1),\n",
              " ('juicehighy', 1),\n",
              " ('recommended', 1),\n",
              " ('mins', 1),\n",
              " ('phonebattery', 1),\n",
              " ('short', 1),\n",
              " ('pics', 1),\n",
              " ('garbage', 1),\n",
              " ('mind', 1),\n",
              " ('gonna', 1),\n",
              " ('arguing', 1),\n",
              " ('bulky', 1),\n",
              " ('usable', 1),\n",
              " ('realworld', 1),\n",
              " ('useful', 1),\n",
              " ('machine', 1),\n",
              " ('neat', 1),\n",
              " ('gadget', 1),\n",
              " ('reasonable', 1),\n",
              " ('ie', 1),\n",
              " ('stream', 1),\n",
              " ('submerged', 1),\n",
              " ('complaints', 1),\n",
              " ('microsofts', 1),\n",
              " ('faceplates', 1),\n",
              " ('elegant', 1),\n",
              " ('angle', 1),\n",
              " ('drawback', 1),\n",
              " ('pause', 1),\n",
              " ('skip', 1),\n",
              " ('songs', 1),\n",
              " ('activated', 1),\n",
              " ('suddenly', 1),\n",
              " ('ipods', 1),\n",
              " ('situations', 1),\n",
              " ('bmw', 1),\n",
              " ('fairly', 1),\n",
              " ('hearing', 1),\n",
              " ('person', 1),\n",
              " ('dwrongly', 1),\n",
              " ('d', 1),\n",
              " ('everyday', 1),\n",
              " ('intended', 1),\n",
              " ('runs', 1),\n",
              " ('boy', 1),\n",
              " ('loads', 1),\n",
              " ('greater', 1),\n",
              " ('buds', 1),\n",
              " ('waaay', 1),\n",
              " ('bluetooths', 1),\n",
              " ('listener', 1),\n",
              " ('integrated', 1),\n",
              " ('seamlessly', 1),\n",
              " ('buyer', 1),\n",
              " ('flush', 1),\n",
              " ('toilet', 1),\n",
              " ('supposedly', 1),\n",
              " ('apparently', 1),\n",
              " ('prosgood', 1),\n",
              " ('styles', 1),\n",
              " ('correctly', 1),\n",
              " ('rated', 1),\n",
              " ('megapixels', 1),\n",
              " ('renders', 1),\n",
              " ('images', 1),\n",
              " ('expectations', 1),\n",
              " ('relatively', 1),\n",
              " ('purcashed', 1),\n",
              " ('geeky', 1),\n",
              " ('toast', 1),\n",
              " ('oozes', 1),\n",
              " ('embedded', 1),\n",
              " ('stylish', 1),\n",
              " ('compromise', 1),\n",
              " ('qwerty', 1),\n",
              " ('basic', 1),\n",
              " ('winner', 1),\n",
              " ('simpler', 1),\n",
              " ('iam', 1),\n",
              " ('disapoinment', 1),\n",
              " ('realize', 1),\n",
              " ('accompanied', 1),\n",
              " ('brilliant', 1),\n",
              " ('nicely', 1),\n",
              " ('damage', 1),\n",
              " ('definitly', 1),\n",
              " ('buyerbe', 1),\n",
              " ('majority', 1),\n",
              " ('peachykeen', 1),\n",
              " ('upstairs', 1),\n",
              " ('basement', 1),\n",
              " ('minute', 1),\n",
              " ('reccomendation', 1),\n",
              " ('relative', 1),\n",
              " ('items', 1),\n",
              " ('sudden', 1),\n",
              " ('linking', 1),\n",
              " ('curve', 1),\n",
              " ('funny', 1),\n",
              " ('seemed', 1),\n",
              " ('sketchy', 1),\n",
              " ('wellwell', 1),\n",
              " ('messages', 1),\n",
              " ('web', 1),\n",
              " ('browsing', 1),\n",
              " ('significantly', 1),\n",
              " ('faster', 1),\n",
              " ('build', 1),\n",
              " ('unlike', 1),\n",
              " ('colors', 1),\n",
              " ('whine', 1),\n",
              " ('goesthe', 1),\n",
              " ('communications', 1),\n",
              " ('communicate', 1),\n",
              " ('monkeys', 1),\n",
              " ('shouldnt', 1),\n",
              " ('share', 1),\n",
              " ('dna', 1),\n",
              " ('copy', 1),\n",
              " ('humans', 1),\n",
              " ('bougth', 1),\n",
              " ('lc', 1),\n",
              " ('mode', 1),\n",
              " ('wasting', 1),\n",
              " ('bethe', 1),\n",
              " ('file', 1),\n",
              " ('browser', 1),\n",
              " ('offers', 1),\n",
              " ('options', 1),\n",
              " ('needshandsfree', 1),\n",
              " ('bluetoothmotorola', 1),\n",
              " ('hs', 1),\n",
              " ('whether', 1),\n",
              " ('latest', 1),\n",
              " ('os', 1),\n",
              " ('vg', 1),\n",
              " ('likes', 1),\n",
              " ('crawl', 1),\n",
              " ('recognizes', 1),\n",
              " ('bluetoooth', 1),\n",
              " ('thorn', 1),\n",
              " ('abhor', 1),\n",
              " ('recently', 1),\n",
              " ('disconnected', 1),\n",
              " ('bucks', 1),\n",
              " ('check', 1),\n",
              " ('mail', 1),\n",
              " ('night', 1),\n",
              " ('backlight', 1),\n",
              " ('message', 1),\n",
              " ('toneoverall', 1),\n",
              " ('lately', 1),\n",
              " ('wit', 1),\n",
              " ('hit', 1),\n",
              " ('weight', 1),\n",
              " ('hardly', 1),\n",
              " ('youll', 1),\n",
              " ('pleather', 1),\n",
              " ('deaf', 1),\n",
              " ('prettier', 1),\n",
              " ('incredibly', 1),\n",
              " ('investment', 1),\n",
              " ('strange', 1),\n",
              " ('ticking', 1),\n",
              " ('noises', 1),\n",
              " ('ends', 1),\n",
              " ('electronics', 1),\n",
              " ('available', 1),\n",
              " ('fm', 1),\n",
              " ('transmitters', 1),\n",
              " ('h', 1),\n",
              " ('mega', 1),\n",
              " ('pixel', 1),\n",
              " ('transmit', 1),\n",
              " ('contacting', 1),\n",
              " ('dollar', 1),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeWWoAA1aE9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df9f8e71-81f2-4ecc-f239-2f6650b187af"
      },
      "source": [
        "# Creats a vocabulary where each number corresponds to a word.\n",
        "\n",
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "\n",
        "# Changes all word in reviews to interger values accoriding to vocab_to_int\n",
        "reviews_int = []\n",
        "for review in reviews:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_int.append(r)\n",
        "print (reviews_int[0:3])\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[33, 115, 5, 58, 210, 11, 50, 8, 153, 4, 19, 326, 19, 1, 533, 534, 2, 234, 185, 6, 786], [21, 48, 54, 262], [16, 11, 1, 403]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOholpuccLTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "5101cd53-9928-4420-bd2d-283360428957"
      },
      "source": [
        "encoded_labels = np.array(labels)\n",
        "encoded_labels"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ0fI6kwcecc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "80920ddd-587d-4e99-aec2-2cbdc04c6db9"
      },
      "source": [
        "#Prints a visualisation of how many word there are in each review\n",
        "reviews_len = [len(x) for x in reviews_int]\n",
        "pd.Series(reviews_len).hist()\n",
        "plt.show()\n",
        "pd.Series(reviews_len).describe()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1000.000000\n",
              "mean       10.118000\n",
              "std         6.598456\n",
              "min         1.000000\n",
              "25%         5.000000\n",
              "50%         9.000000\n",
              "75%        15.000000\n",
              "max        30.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz0hPwg5dNm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removes all reviews which have zero words in them.\n",
        "\n",
        "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
        "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0 ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt5W47i_dzjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fucntion which zero padds and shortens all reviews to seq_length\n",
        "\n",
        "def pad_features(reviews_int, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
        "    \n",
        "    for i, review in enumerate(reviews_int):\n",
        "        review_len = len(review)\n",
        "        \n",
        "        if review_len <= seq_length:\n",
        "            zeroes = list(np.zeros(seq_length-review_len))\n",
        "            new = zeroes+review        \n",
        "        elif review_len > seq_length:\n",
        "            new = review[0:seq_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTYuUWHkeK7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04a8c28b-f51d-4b27-a82a-6e3079320c75"
      },
      "source": [
        "features = np.array(pad_features(reviews_int,200))\n",
        "features[10][:20]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCFysaaOekaU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74c6fb80-fd7c-42b9-d4f3-1649d8a0b245"
      },
      "source": [
        "# SPlitting data into training, validation and testing\n",
        "split_frac = 0.6\n",
        "len_feat=len(features)\n",
        "encoded_labels = np.array(encoded_labels)\n",
        "\n",
        "train_x = features[0:int(split_frac*len_feat)]\n",
        "train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
        "remaining_x = features[int(split_frac*len_feat):]\n",
        "remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
        "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
        "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
        "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
        "test_y = remaining_y[int(len(remaining_y)*0.5):]\n",
        "\n",
        "\n",
        "type(train_y)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IsHVEUofTLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbvlbYutgkMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "4829c22f-0154-40ab-96b7-3c8af8f35232"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) \n",
        "# batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) \n",
        "# batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[  0,   0,   0,  ...,  81,  15, 111],\n",
            "        [  0,   0,   0,  ..., 109, 201, 317],\n",
            "        [  0,   0,   0,  ...,  11, 858,  74],\n",
            "        ...,\n",
            "        [  0,   0,   0,  ...,  59,  15,  39],\n",
            "        [  0,   0,   0,  ..., 375,   7,  24],\n",
            "        [  0,   0,   0,  ..., 225,  11, 542]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTqdw8SIrPj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddb68e5e-0c96-4566-d065-d5f306816b96"
      },
      "source": [
        "vocab_size = len(words)\n",
        "embedding_dim = 30\n",
        "embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "print ('Embedding layer is ', embeds)\n",
        "print ('Embedding layer weights ', embeds.weight.shape)\n",
        "\n",
        "embeds_out = embeds(sample_x)\n",
        "print ('Embedding layer output shape', embeds_out.shape)\n",
        "print ('Embedding layer output ', embeds_out)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding layer is  Embedding(10118, 30)\n",
            "Embedding layer weights  torch.Size([10118, 30])\n",
            "Embedding layer output shape torch.Size([50, 200, 30])\n",
            "Embedding layer output  tensor([[[ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         ...,\n",
            "         [-1.0275e+00, -2.0627e-01,  1.1695e+00,  ..., -9.1235e-01,\n",
            "          -5.4317e-01, -2.1056e+00],\n",
            "         [-3.3015e-01,  1.0118e+00, -1.3435e+00,  ..., -4.8249e-01,\n",
            "           1.0138e-01,  6.8744e-01],\n",
            "         [-3.3150e+00,  6.8687e-01, -8.0308e-01,  ..., -1.9401e+00,\n",
            "          -2.1702e-01,  6.5867e-01]],\n",
            "\n",
            "        [[ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         ...,\n",
            "         [ 1.2407e+00,  7.9178e-01,  5.8878e-01,  ..., -1.9995e+00,\n",
            "          -3.2529e-01, -2.2673e-01],\n",
            "         [-5.0676e-01,  1.3369e+00,  5.4905e-01,  ...,  1.0191e-01,\n",
            "           1.7389e+00,  2.3707e+00],\n",
            "         [-9.7757e-01, -5.7454e-01, -1.6229e+00,  ...,  2.6383e-01,\n",
            "          -1.0157e+00,  2.8755e-01]],\n",
            "\n",
            "        [[ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         ...,\n",
            "         [-1.7130e+00,  2.1453e+00, -1.3477e+00,  ...,  3.5302e-01,\n",
            "          -1.2160e+00, -1.1856e-01],\n",
            "         [-1.3681e-01,  1.1524e+00, -4.3207e-01,  ..., -1.5786e+00,\n",
            "           1.3486e+00, -5.2440e-01],\n",
            "         [ 3.8900e-01, -7.6672e-02,  8.1157e-01,  ..., -1.8491e+00,\n",
            "           6.3004e-01,  1.2862e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         ...,\n",
            "         [ 1.2301e+00,  2.0323e-01, -4.8737e-01,  ...,  6.2729e-02,\n",
            "          -5.7052e-01,  7.3733e-01],\n",
            "         [-3.3015e-01,  1.0118e+00, -1.3435e+00,  ..., -4.8249e-01,\n",
            "           1.0138e-01,  6.8744e-01],\n",
            "         [ 6.7227e-01, -1.8819e+00,  1.4215e+00,  ...,  1.2680e+00,\n",
            "           6.7573e-01, -4.0830e-01]],\n",
            "\n",
            "        [[ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         ...,\n",
            "         [-3.5200e-01, -7.1304e-01,  1.6656e+00,  ..., -3.0080e-01,\n",
            "           7.4343e-01, -3.6904e-01],\n",
            "         [-5.8852e-01,  2.1094e+00,  1.6210e+00,  ...,  7.4430e-01,\n",
            "          -1.2170e-02,  7.5592e-01],\n",
            "         [-6.6402e-01, -1.2786e+00, -9.1077e-01,  ..., -2.1036e-01,\n",
            "          -1.7848e+00, -1.3863e+00]],\n",
            "\n",
            "        [[ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         [ 2.5555e-01,  2.0309e+00, -1.2330e+00,  ...,  2.4847e-01,\n",
            "          -9.9718e-01,  9.9722e-01],\n",
            "         ...,\n",
            "         [ 1.0990e+00,  2.7553e-03, -4.5835e-01,  ...,  8.9675e-01,\n",
            "           9.6192e-01, -2.4279e+00],\n",
            "         [-1.7130e+00,  2.1453e+00, -1.3477e+00,  ...,  3.5302e-01,\n",
            "          -1.2160e+00, -1.1856e-01],\n",
            "         [ 1.0251e+00, -1.9130e+00,  9.5874e-01,  ...,  7.5013e-02,\n",
            "           3.4160e-01,  2.2504e-01]]], grad_fn=<EmbeddingBackward>)\n",
            "LSTM layer output shape torch.Size([50, 200, 512])\n",
            "LSTM layer output  tensor([[[ 0.0024, -0.0095, -0.0008,  ...,  0.0135, -0.0176,  0.0450],\n",
            "         [ 0.0033, -0.0175,  0.0105,  ...,  0.0282, -0.0189,  0.0644],\n",
            "         [ 0.0024, -0.0242,  0.0211,  ...,  0.0399, -0.0164,  0.0730],\n",
            "         ...,\n",
            "         [ 0.0314, -0.0689,  0.0269,  ..., -0.0289,  0.0427, -0.0505],\n",
            "         [ 0.1109,  0.0244, -0.0363,  ...,  0.0456,  0.0935, -0.0090],\n",
            "         [ 0.1109, -0.0830, -0.0044,  ...,  0.0302,  0.1237, -0.0157]],\n",
            "\n",
            "        [[ 0.0024, -0.0095, -0.0008,  ...,  0.0135, -0.0176,  0.0450],\n",
            "         [ 0.0033, -0.0175,  0.0105,  ...,  0.0282, -0.0189,  0.0644],\n",
            "         [ 0.0024, -0.0242,  0.0211,  ...,  0.0399, -0.0164,  0.0730],\n",
            "         ...,\n",
            "         [ 0.0168, -0.0601,  0.0772,  ...,  0.0075, -0.0434,  0.0304],\n",
            "         [ 0.0150,  0.0151,  0.0464,  ...,  0.0040, -0.0538,  0.0230],\n",
            "         [ 0.0178, -0.0270,  0.1001,  ...,  0.0595,  0.0876,  0.0229]],\n",
            "\n",
            "        [[ 0.0024, -0.0095, -0.0008,  ...,  0.0135, -0.0176,  0.0450],\n",
            "         [ 0.0033, -0.0175,  0.0105,  ...,  0.0282, -0.0189,  0.0644],\n",
            "         [ 0.0024, -0.0242,  0.0211,  ...,  0.0399, -0.0164,  0.0730],\n",
            "         ...,\n",
            "         [-0.0112, -0.0859,  0.0738,  ...,  0.0596,  0.0207,  0.0382],\n",
            "         [ 0.0144, -0.1168,  0.0703,  ..., -0.0416,  0.0160, -0.0190],\n",
            "         [ 0.0411, -0.0406,  0.0308,  ...,  0.0243,  0.0096,  0.0008]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0024, -0.0095, -0.0008,  ...,  0.0135, -0.0176,  0.0450],\n",
            "         [ 0.0033, -0.0175,  0.0105,  ...,  0.0282, -0.0189,  0.0644],\n",
            "         [ 0.0024, -0.0242,  0.0211,  ...,  0.0399, -0.0164,  0.0730],\n",
            "         ...,\n",
            "         [-0.0255, -0.0135,  0.0082,  ..., -0.0022, -0.0150,  0.0231],\n",
            "         [ 0.0856,  0.0355, -0.0297,  ...,  0.0450,  0.0608,  0.0445],\n",
            "         [-0.0245, -0.0108,  0.0197,  ...,  0.0053,  0.0440,  0.0240]],\n",
            "\n",
            "        [[ 0.0024, -0.0095, -0.0008,  ...,  0.0135, -0.0176,  0.0450],\n",
            "         [ 0.0033, -0.0175,  0.0105,  ...,  0.0282, -0.0189,  0.0644],\n",
            "         [ 0.0024, -0.0242,  0.0211,  ...,  0.0399, -0.0164,  0.0730],\n",
            "         ...,\n",
            "         [-0.0004,  0.0398,  0.0935,  ..., -0.0212, -0.0038, -0.0499],\n",
            "         [ 0.0281,  0.0760,  0.0551,  ..., -0.0276, -0.0852,  0.0234],\n",
            "         [-0.0161,  0.0129,  0.0694,  ...,  0.0027, -0.0179,  0.0427]],\n",
            "\n",
            "        [[ 0.0024, -0.0095, -0.0008,  ...,  0.0135, -0.0176,  0.0450],\n",
            "         [ 0.0033, -0.0175,  0.0105,  ...,  0.0282, -0.0189,  0.0644],\n",
            "         [ 0.0024, -0.0242,  0.0211,  ...,  0.0399, -0.0164,  0.0730],\n",
            "         ...,\n",
            "         [-0.0602, -0.1155,  0.1007,  ..., -0.0035,  0.0254, -0.0536],\n",
            "         [-0.0264, -0.0924,  0.0836,  ...,  0.0378,  0.0161,  0.0200],\n",
            "         [-0.0424, -0.0242,  0.0632,  ...,  0.0181,  0.0249, -0.0656]]],\n",
            "       grad_fn=<TransposeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCE-KNdtsI8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "db7fb314-bde8-458e-b174-7b1dd3cab751"
      },
      "source": [
        "# initializing the hidden state to 0\n",
        "hidden=None\n",
        "lstm = nn.LSTM(input_size=embedding_dim, hidden_size=512, num_layers=1, batch_first=True)\n",
        "lstm_out, h = lstm(embeds_out, hidden)\n",
        "print ('LSTM layer output shape', lstm_out.shape)\n",
        "print ('LSTM layer output ', lstm_out)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM layer output shape torch.Size([50, 200, 512])\n",
            "LSTM layer output  tensor([[[ 0.0045,  0.0380,  0.0113,  ...,  0.0020,  0.0396, -0.0003],\n",
            "         [ 0.0049,  0.0543,  0.0187,  ...,  0.0013,  0.0602, -0.0046],\n",
            "         [ 0.0027,  0.0605,  0.0235,  ..., -0.0002,  0.0723, -0.0096],\n",
            "         ...,\n",
            "         [-0.0780, -0.0215,  0.0015,  ..., -0.0352,  0.0486, -0.0115],\n",
            "         [ 0.0259,  0.0049,  0.0529,  ...,  0.0323,  0.1022, -0.0257],\n",
            "         [ 0.0105,  0.0264,  0.0202,  ...,  0.0290,  0.0894, -0.0314]],\n",
            "\n",
            "        [[ 0.0045,  0.0380,  0.0113,  ...,  0.0020,  0.0396, -0.0003],\n",
            "         [ 0.0049,  0.0543,  0.0187,  ...,  0.0013,  0.0602, -0.0046],\n",
            "         [ 0.0027,  0.0605,  0.0235,  ..., -0.0002,  0.0723, -0.0096],\n",
            "         ...,\n",
            "         [ 0.0420,  0.0918,  0.0079,  ..., -0.0069,  0.0548, -0.0413],\n",
            "         [ 0.0085,  0.0328,  0.0725,  ..., -0.0199,  0.0609, -0.0224],\n",
            "         [ 0.0057,  0.0688,  0.0522,  ..., -0.0472,  0.0718,  0.0134]],\n",
            "\n",
            "        [[ 0.0045,  0.0380,  0.0113,  ...,  0.0020,  0.0396, -0.0003],\n",
            "         [ 0.0049,  0.0543,  0.0187,  ...,  0.0013,  0.0602, -0.0046],\n",
            "         [ 0.0027,  0.0605,  0.0235,  ..., -0.0002,  0.0723, -0.0096],\n",
            "         ...,\n",
            "         [-0.0012, -0.0266,  0.0392,  ...,  0.0026,  0.1092, -0.0463],\n",
            "         [ 0.0211, -0.0508,  0.0066,  ...,  0.0197,  0.0834, -0.0248],\n",
            "         [ 0.0214,  0.0365, -0.0376,  ...,  0.0113,  0.0784, -0.0048]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0045,  0.0380,  0.0113,  ...,  0.0020,  0.0396, -0.0003],\n",
            "         [ 0.0049,  0.0543,  0.0187,  ...,  0.0013,  0.0602, -0.0046],\n",
            "         [ 0.0027,  0.0605,  0.0235,  ..., -0.0002,  0.0723, -0.0096],\n",
            "         ...,\n",
            "         [-0.0231,  0.0549,  0.0534,  ..., -0.0225,  0.0954, -0.0040],\n",
            "         [ 0.0463,  0.0300,  0.0708,  ...,  0.0324,  0.1198, -0.0299],\n",
            "         [ 0.0241, -0.0346,  0.0599,  ..., -0.0747,  0.0549, -0.0528]],\n",
            "\n",
            "        [[ 0.0045,  0.0380,  0.0113,  ...,  0.0020,  0.0396, -0.0003],\n",
            "         [ 0.0049,  0.0543,  0.0187,  ...,  0.0013,  0.0602, -0.0046],\n",
            "         [ 0.0027,  0.0605,  0.0235,  ..., -0.0002,  0.0723, -0.0096],\n",
            "         ...,\n",
            "         [-0.0057,  0.0059,  0.0233,  ..., -0.0457,  0.0082,  0.0153],\n",
            "         [ 0.0204,  0.0064,  0.0707,  ..., -0.1009,  0.0314, -0.0394],\n",
            "         [ 0.0197, -0.0316, -0.0165,  ..., -0.0401,  0.0098, -0.0541]],\n",
            "\n",
            "        [[ 0.0045,  0.0380,  0.0113,  ...,  0.0020,  0.0396, -0.0003],\n",
            "         [ 0.0049,  0.0543,  0.0187,  ...,  0.0013,  0.0602, -0.0046],\n",
            "         [ 0.0027,  0.0605,  0.0235,  ..., -0.0002,  0.0723, -0.0096],\n",
            "         ...,\n",
            "         [ 0.0067, -0.0091, -0.0063,  ..., -0.0538, -0.0538, -0.0271],\n",
            "         [ 0.0184, -0.0272,  0.0335,  ..., -0.0176,  0.0012, -0.0372],\n",
            "         [ 0.0630, -0.0387,  0.0058,  ..., -0.0308, -0.0307, -0.0332]]],\n",
            "       grad_fn=<TransposeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQuPP3ERsQXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "676f0a8d-5de0-4358-8c48-ac43b76f8b1c"
      },
      "source": [
        "fc = nn.Linear(in_features=512, out_features=1)\n",
        "fc_out = fc(lstm_out.contiguous().view(-1, 512))\n",
        "print ('FC layer output shape', fc_out.shape)\n",
        "print ('FC layer output ', fc_out)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FC layer output shape torch.Size([10000, 1])\n",
            "FC layer output  tensor([[-0.0333],\n",
            "        [-0.0391],\n",
            "        [-0.0449],\n",
            "        ...,\n",
            "        [-0.0703],\n",
            "        [-0.0721],\n",
            "        [-0.0571]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX0vfh7jslK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "060cf11d-0370-4202-a5f3-1f38fcf113e6"
      },
      "source": [
        "sigm = nn.Sigmoid()\n",
        "sigm_out = sigm(fc_out)\n",
        "print ('Sigmoid layer output shape', sigm_out.shape)\n",
        "print ('Sigmoid layer output ', sigm_out)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sigmoid layer output shape torch.Size([10000, 1])\n",
            "Sigmoid layer output  tensor([[0.4917],\n",
            "        [0.4902],\n",
            "        [0.4888],\n",
            "        ...,\n",
            "        [0.4824],\n",
            "        [0.4820],\n",
            "        [0.4857]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrP6fy8XstG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "bb594c53-acf5-41d9-d6a1-956187ed9a74"
      },
      "source": [
        "batch_size = sample_x.shape[0]\n",
        "out = sigm_out.view(batch_size, -1)\n",
        "print ('Output layer output shape', out.shape)\n",
        "print ('Output layer output ', out)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output layer output shape torch.Size([50, 200])\n",
            "Output layer output  tensor([[0.4917, 0.4902, 0.4888,  ..., 0.4869, 0.4776, 0.4782],\n",
            "        [0.4917, 0.4902, 0.4888,  ..., 0.4848, 0.4869, 0.4761],\n",
            "        [0.4917, 0.4902, 0.4888,  ..., 0.4797, 0.4939, 0.4861],\n",
            "        ...,\n",
            "        [0.4917, 0.4902, 0.4888,  ..., 0.4753, 0.4714, 0.4807],\n",
            "        [0.4917, 0.4902, 0.4888,  ..., 0.4896, 0.4900, 0.4901],\n",
            "        [0.4917, 0.4902, 0.4888,  ..., 0.4824, 0.4820, 0.4857]],\n",
            "       grad_fn=<ViewBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpkM7Esos5p9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "b9c87475-a3ca-4b9d-dafd-f52bbfbeb2c0"
      },
      "source": [
        "print ('Final sentiment prediction, ', out[:,-1])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final sentiment prediction,  tensor([0.4782, 0.4761, 0.4861, 0.4878, 0.4922, 0.4892, 0.4928, 0.4948, 0.4913,\n",
            "        0.4917, 0.4994, 0.4896, 0.4911, 0.4930, 0.4895, 0.4931, 0.4854, 0.4876,\n",
            "        0.5046, 0.4795, 0.4897, 0.4838, 0.4927, 0.4892, 0.4985, 0.4932, 0.5000,\n",
            "        0.4822, 0.4755, 0.4886, 0.4950, 0.4850, 0.4897, 0.4817, 0.4866, 0.4763,\n",
            "        0.4793, 0.4877, 0.4925, 0.4910, 0.4941, 0.5016, 0.4903, 0.4796, 0.4856,\n",
            "        0.4929, 0.4867, 0.4807, 0.4901, 0.4857], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPz3vujPrLNN",
        "colab_type": "text"
      },
      "source": [
        "asd\n",
        "s\n",
        "\n",
        "s\n",
        "s\n",
        "s\n",
        "\n",
        "s\n",
        "s\n",
        "s\n",
        "s\n",
        "s\n",
        "\n",
        "s\n",
        "s\n",
        "s\n",
        "\n",
        "s\n",
        "\n",
        "s\n",
        "s\n",
        "s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaV-20eRKVaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "8fbb00a1-fe4d-46b3-ee03-641a93a45893"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load embeddings from the pre-trained file\n",
        "# The file can be found in Canvas and must be placed in the same directory as the notebook)\n",
        "fastText_embeddings = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Chatbot_shared/wiki.simple.vec')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KKBU_J9b4GL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "b1bec31d-2bcf-4c3f-f0be-2df97fbf9d35"
      },
      "source": [
        "word='sweden'\n",
        "#print(words[0])\n",
        "test1=fastText_embeddings['sweden']\n",
        "test2=fastText_embeddings['norway']\n",
        "\n",
        "print(len(test1))\n",
        "from scipy import spatial\n",
        "\n",
        "result = 1 - spatial.distance.cosine(test1, test2)\n",
        "\n",
        "print(result)\n",
        "\n",
        "fastText_embeddings.most_similar(positive=['sweden'])\n",
        "\n",
        "print(fastText_embeddings.wv.vectors.shape)\n",
        "\n",
        "weights = torch.FloatTensor(fastText_embeddings.vectors)\n",
        "\n",
        "print(weights)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "0.597154974937439\n",
            "(111051, 300)\n",
            "tensor([[ 0.2892, -0.4608,  0.3514,  ...,  0.1331, -0.2347,  0.0053],\n",
            "        [ 0.0569, -0.0520,  0.2733,  ..., -0.0695, -0.1606, -0.0989],\n",
            "        [ 0.2013,  0.0104,  0.1623,  ..., -0.0931, -0.1408, -0.1326],\n",
            "        ...,\n",
            "        [ 0.2377, -0.0403, -0.0326,  ..., -0.0606,  0.0348, -0.2974],\n",
            "        [ 0.5376, -0.2263,  0.0231,  ...,  0.0800, -0.3782, -0.2765],\n",
            "        [ 0.1598,  0.0028, -0.2026,  ..., -0.0032, -0.1265, -0.1526]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBGMs2sUVvxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4a17802-cb8b-422b-8b7c-b47b81f4f237"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgX8FYVBPaJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J89i_NyxhysH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab_to_int) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "\n",
        "lr=0.005\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szZWJ3I-UzV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "67dc4143-5762-4fd0-9864-936260903d00"
      },
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 1\n",
        "counter = 0\n",
        "print_every = 6\n",
        "clip = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train()\n",
        "for i in (range(epochs)):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "\n",
        "    for inputs, labels in (train_loader):\n",
        "        counter += 1\n",
        "        h = tuple([e.data for e in h])\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        if counter%print_every == 0:\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inp, lab in valid_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                inp, lab = inp.to(device), lab.to(device)\n",
        "                out, val_h = model(inp, val_h)\n",
        "                val_loss = criterion(out.squeeze(), lab.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            if np.mean(val_losses) <= valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-8feb8e5d9a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NELl8_3kZDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a9fd7efe-0ea7-4422-a3c3-0dff98c7c66a"
      },
      "source": [
        "# Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))\n",
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.592\n",
            "Test accuracy: 71.000%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}